{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbabded9-cf8c-4833-a4be-aab5cb6f98af",
   "metadata": {},
   "source": [
    "# Pose Landmarks with MediaPipe ‚Äî From Local Videos & Folders Using Python\n",
    "\n",
    "This notebook is both a **guided lesson** and a **working pipeline** for detecting human pose landmarks from **local video files** or **entire folders** of videos using **MediaPipe Tasks**.\n",
    "\n",
    "## Goal\n",
    "\n",
    "1. Set up a clean Python 3.12 environment and verify required packages.\n",
    "2. Understand each step and terminologies.\n",
    "3. Download and select a Pose Landmarker model (**lite / full / heavy**) and understand accuracy‚Äìspeed trade-offs.\n",
    "4. Read videos with OpenCV and run inference in **`RunningMode.VIDEO`** with correct **timestamps**.\n",
    "5. Export results as tidy CSVs for analysis: **2D image-normalized** and **3D world** landmarks.\n",
    "6. Create an **annotated MP4** showing the skeleton overlay.\n",
    "7. Build intuition for **visibility**, **image vs. world coordinates**, and simple feature engineering (e.g., joint angles).\n",
    "\n",
    "> **Built for learning:** Along the way you‚Äôll see short callouts explaining *why* each step exists (e.g., timestamps in VIDEO mode), how coordinate spaces differ, and how to tune speed vs. accuracy.\n",
    "\n",
    "## After completing this guide, you will be able to\n",
    "\n",
    "* Load one video‚Äîor loop through an entire folder‚Äîand extract the coordinates of the landmark bodypoints frame-by-frame.\n",
    "* Save two analysis-ready CSVs per video: one for **2D normalized** landmarks and one for **3D world** coordinates.\n",
    "* Produce an **annotated MP4** with landmarks and connections overlaid.\n",
    "* Explain and adjust **`RunningMode.VIDEO`**, **per-frame timestamps**, **visibility filtering**, **image vs. world coordinates**, and model variants (**lite/full/heavy**).\n",
    "\n",
    "> **Prerequisites**\n",
    ">\n",
    "> * Python **3.12** virtual environment selected as the active Jupyter kernel. In case yo8u need help, please refer to the \"LS100_Guide 3_Introduction to Pose Estimation Using MediaPipe.pdf\" guide.\n",
    "> * Installed packages: `mediapipe opencv-python pandas numpy tqdm matplotlib seaborn`\n",
    "> * One or more local video files (e.g., `.mp4`) to test.\n",
    "\n",
    "> **Ethics & consent**\n",
    ">\n",
    "> * If processing videos of people, obtain consent and store data securely. Avoid uploading sensitive content to third-party services.\n",
    "\n",
    "### References for learners\n",
    "\n",
    "* MediaPipe Pose Landmarker (Python guide): [https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker/python](https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker/python)\n",
    "* Pose Landmarker API: [https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/PoseLandmarker](https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/PoseLandmarker)\n",
    "* Model card (BlazePose GHUM 3D; lite/full/heavy):\n",
    "  [https://storage.googleapis.com/mediapipe-assets/Model%20Card%20BlazePose%20GHUM%203D.pdf](https://storage.googleapis.com/mediapipe-assets/Model%20Card%20BlazePose%20GHUM%203D.pdf)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf62f41-3483-4103-9b1e-51d470dc174d",
   "metadata": {},
   "source": [
    "# 0. Environment Setup and Verification (LS100 Standard)\n",
    "\n",
    "Before running any code, make sure you‚Äôre using the **LS100_PoseEstimation_MP** kernel that was created in your Python 3.12 virtual environment.\n",
    "This section verifies your environment and installs all required packages.\n",
    "\n",
    "---\n",
    "\n",
    "### **What you should already have**\n",
    "\n",
    "‚úÖ Python 3.12 installed\n",
    "\n",
    "‚úÖ Virtual environment activated (`(MediaPipeEnv)` should appear in your terminal)\n",
    "\n",
    "‚úÖ Kernel registered as **LS100_PoseEstimation_MP**\n",
    "\n",
    "If you haven‚Äôt completed those steps, revisit the **LS100_Guide 3_Introduction to Pose Estimation Using MediaPipe.pdf** document.\n",
    "\n",
    "---\n",
    "\n",
    "### **Required packages**\n",
    "\n",
    "This notebook uses the following libraries:\n",
    "\n",
    "* `mediapipe` ‚Äì pose landmark model and API\n",
    "* `opencv-python` ‚Äì video I/O (input/output) and frame conversion\n",
    "* `pandas` & `numpy` ‚Äì data handling and analysis\n",
    "* `tqdm` ‚Äì progress bars for video processing\n",
    "* `matplotlib` & `seaborn` ‚Äì visualization and data inspection\n",
    "\n",
    "Run the next cell to ensure these are installed and to confirm the environment details.\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning focus**\n",
    "\n",
    "* Why virtual environments prevent version conflicts\n",
    "* Why we require **Python 3.12** (MediaPipe Tasks currently supports Python 3.9‚Äì3.12 only)\n",
    "* How each library fits into the MediaPipe Pose pipeline\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222fe87d",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Environment setup\n",
    "\n",
    "> If running locally (VS Code/Jupyter), run the following cell once; it might take about a minute to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64456878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Python version: 3.12.12\n",
      "‚úÖ mediapipe already installed\n",
      "‚¨áÔ∏è Installing opencv-python ...\n",
      "‚úÖ pandas already installed\n",
      "‚úÖ numpy already installed\n",
      "‚úÖ tqdm already installed\n",
      "‚úÖ matplotlib already installed\n",
      "‚úÖ seaborn already installed\n",
      "\n",
      "üì¶ Package versions:\n",
      "mediapipe      : 0.10.21\n",
      "opencv-python  : 4.11.0\n",
      "pandas         : 2.3.3\n",
      "numpy          : 1.26.4\n",
      "matplotlib     : 3.10.7\n",
      "seaborn        : 0.13.2\n",
      "\n",
      "‚úÖ Environment is ready to proceed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 0. Environment Setup and Package Verification\n",
    "# ============================================\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "# ---- 1. Check Python version ----\n",
    "py_version = sys.version_info\n",
    "print(f\"üß† Python version: {py_version.major}.{py_version.minor}.{py_version.micro}\")\n",
    "if py_version < (3, 9) or py_version >= (3, 13):\n",
    "    print(\"‚ö†Ô∏è MediaPipe Tasks officially supports Python 3.9‚Äì3.12.\")\n",
    "    print(\"‚ö†Ô∏è Please switch to Python 3.12 for this notebook (as used in LS100).\")\n",
    "\n",
    "# ---- 2. Define required packages ----\n",
    "required_packages = [\n",
    "    \"mediapipe\",\n",
    "    \"opencv-python\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"tqdm\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "]\n",
    "\n",
    "# ---- 3. Function to check and install ----\n",
    "def install_if_missing(pkg):\n",
    "    \"\"\"\n",
    "    Try importing the package; if not found, install it quietly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        importlib.import_module(pkg.split(\"==\")[0])\n",
    "        print(f\"‚úÖ {pkg} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"‚¨áÔ∏è Installing {pkg} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "# ---- 4. Verify each dependency ----\n",
    "for package in required_packages:\n",
    "    install_if_missing(package)\n",
    "\n",
    "# ---- 5. Print package versions for reproducibility ----\n",
    "import mediapipe as mp\n",
    "import cv2, pandas as pd, numpy as np, tqdm, matplotlib, seaborn\n",
    "\n",
    "print(\"\\nüì¶ Package versions:\")\n",
    "print(f\"mediapipe      : {mp.__version__}\")\n",
    "print(f\"opencv-python  : {cv2.__version__}\")\n",
    "print(f\"pandas         : {pd.__version__}\")\n",
    "print(f\"numpy          : {np.__version__}\")\n",
    "print(f\"matplotlib     : {matplotlib.__version__}\")\n",
    "print(f\"seaborn        : {seaborn.__version__}\")\n",
    "\n",
    "print(\"\\n‚úÖ Environment is ready to proceed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccfbb6",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports & version checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ffc92-aa13-4a39-9a17-6315fc1d1e79",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Imports and Version Verification\n",
    "\n",
    "Now that your environment is ready, let‚Äôs import the main libraries used throughout this notebook.\n",
    "\n",
    "This step helps confirm that:\n",
    "\n",
    "* The correct packages are installed inside your LS100 virtual environment\n",
    "* MediaPipe loads successfully (and we can access its **Tasks API**)\n",
    "* OpenCV, NumPy, and Pandas are working properly\n",
    "\n",
    "If an import fails, it usually means you‚Äôre running the notebook in a different kernel (not the one you registered).\n",
    "You can fix that by selecting **Kernel ‚Üí Change Kernel ‚Üí LS100_PoseEstimation_MP** (or the name you chose).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f6603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MediaPipe Tasks API imported successfully!\n",
      "\n",
      "mediapipe version : 0.10.21\n",
      "opencv version    : 4.11.0\n",
      "pandas version    : 2.3.3\n",
      "numpy version     : 1.26.4\n",
      "‚öôÔ∏è Running on CPU\n",
      "\n",
      " MediaPipe Tasks API is available:\n",
      "- BaseOptions           : True\n",
      "- PoseLandmarker        : True\n",
      "- PoseLandmarkerOptions : True\n",
      "- RunningMode           : True\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 1. Import Libraries and Verify Versions (fixed for MediaPipe >=0.10)\n",
    "# ======================================\n",
    "\n",
    "import os, cv2, numpy as np, pandas as pd, matplotlib, seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python as mp_python\n",
    "from mediapipe.tasks.python import vision as mp_vision\n",
    "\n",
    "print(\"‚úÖ MediaPipe Tasks API imported successfully!\\n\")\n",
    "print(f\"mediapipe version : {mp.__version__}\")\n",
    "print(f\"opencv version    : {cv2.__version__}\")\n",
    "print(f\"pandas version    : {pd.__version__}\")\n",
    "print(f\"numpy version     : {np.__version__}\")\n",
    "\n",
    "# Optional: check GPU availability\n",
    "backend = \"GPU\" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else \"CPU\"\n",
    "print(f\"‚öôÔ∏è Running on {backend}\")\n",
    "\n",
    "# ---- Smoke test: confirm Tasks API symbols exist ----\n",
    "BaseOptions = mp_python.BaseOptions\n",
    "PoseLandmarker = mp_vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp_vision.PoseLandmarkerOptions\n",
    "RunningMode = mp_vision.RunningMode\n",
    "\n",
    "print(\"\\n MediaPipe Tasks API is available:\")\n",
    "print(f\"- BaseOptions           : {BaseOptions is not None}\")\n",
    "print(f\"- PoseLandmarker        : {PoseLandmarker is not None}\")\n",
    "print(f\"- PoseLandmarkerOptions : {PoseLandmarkerOptions is not None}\")\n",
    "print(f\"- RunningMode           : {RunningMode is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d16a0-d5a3-4a65-ae1e-09995f8c0820",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Notes**\n",
    "\n",
    "* **Why this matters:** ensures that the environment is truly isolated and reproducible.\n",
    "* **Discussion prompt:** Can you tell *why* we check MediaPipe imports *before* running the pipeline? (to confirm the **Tasks** API is available and working).\n",
    "* **TASK:** Print `mp.__file__` to confirm MediaPipe‚Äôs path. This helps you understand where packages live inside the venv.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca8f9e",
   "metadata": {},
   "source": [
    "\n",
    "## 2. How Pose Landmarker works\n",
    "\n",
    "- **Running modes:** `IMAGE`, `VIDEO`, `LIVE_STREAM`. For offline videos we use **`VIDEO`** and must pass a **timestamp (ms)** for each frame; the task uses **tracking** to avoid re-running the full model on every frame (reduces latency at the same accuracy settings).  \n",
    "- **Outputs:**  \n",
    "  - **2D normalized landmarks** in image coordinates (*x,y in [0,1] relative to width/height; z is a depth-like value; visibility in [0,1]*).  \n",
    "  - **3D world landmarks** (meters, origin near hip center; handy for biomechanical features).  \n",
    "- **Variants:** **lite / full / heavy**. Heavier models = more accurate, slower (see model card).  \n",
    "- **Accuracy vs speed knobs:** `num_poses` (usually 1 for single-person), `min_pose_detection_confidence`, `min_pose_presence_confidence`, `min_tracking_confidence`, and **frame stride** (e.g., analyze every 2nd/3rd frame).\n",
    "\n",
    "> We‚Äôll expose all of these transparently in helper functions below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69078313",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Download a Pose Landmarker model (`.task` bundle)\n",
    "\n",
    "Choose one of: `\"lite\"`, `\"full\"`, `\"heavy\"` (default).  \n",
    "URLs follow Google‚Äôs published pattern; we try `latest/‚Ä¶` first and then fall back to version `1/‚Ä¶`.\n",
    "\n",
    "> You only need to download once; it will be cached under `models/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70bce5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Model already present: models/pose_landmarker_heavy.task\n",
      "‚úÖ PoseLandmarker initialized successfully (VIDEO mode).\n",
      "   Model: heavy ‚Üí models/pose_landmarker_heavy.task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1761584076.985338 53872720 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M2 Max\n",
      "W0000 00:00:1761584077.082610 53919524 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761584077.170321 53919532 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 3. Model Selection & Download\n",
    "# ================================\n",
    "import os\n",
    "import pathlib\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python as mp_python\n",
    "from mediapipe.tasks.python import vision as mp_vision\n",
    "\n",
    "# ---- Where to save models ----\n",
    "MODELS_DIR = pathlib.Path(\"models\")\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Official model URLs (latest, with fallback to v1) ----\n",
    "MODEL_URLS = {\n",
    "    \"lite\": [\n",
    "        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/latest/pose_landmarker_lite.task\",\n",
    "        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task\",\n",
    "    ],\n",
    "    \"full\": [\n",
    "        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/latest/pose_landmarker_full.task\",\n",
    "        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/1/pose_landmarker_full.task\",\n",
    "    ],\n",
    "    \"heavy\": [\n",
    "        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/latest/pose_landmarker_heavy.task\",\n",
    "        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "def download_pose_model(variant: str = \"heavy\") -> str:\n",
    "    \"\"\"\n",
    "    Download the selected model variant (.task) to MODELS_DIR.\n",
    "    Returns the local file path.\n",
    "    \"\"\"\n",
    "    variant = variant.lower().strip()\n",
    "    assert variant in MODEL_URLS, f\"Unknown variant '{variant}'. Choose: lite, full, heavy.\"\n",
    "\n",
    "    out_path = MODELS_DIR / f\"pose_landmarker_{variant}.task\"\n",
    "    if out_path.exists() and out_path.stat().st_size > 50_000:\n",
    "        print(f\"‚úî Model already present: {out_path}\")\n",
    "        return str(out_path)\n",
    "\n",
    "    last_err = None\n",
    "    for url in MODEL_URLS[variant]:\n",
    "        try:\n",
    "            print(f\"Downloading {variant} model from:\\n  {url}\")\n",
    "            with urllib.request.urlopen(url, timeout=60) as r, open(out_path, \"wb\") as f:\n",
    "                f.write(r.read())\n",
    "            if out_path.stat().st_size <= 50_000:\n",
    "                raise RuntimeError(\"Downloaded file seems too small; trying fallback...\")\n",
    "            print(f\"‚úî Saved to {out_path} ({out_path.stat().st_size/1e6:.2f} MB)\")\n",
    "            return str(out_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚Ä¶ failed: {e}\")\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Could not download model for variant '{variant}'. Last error: {last_err}\")\n",
    "\n",
    "# ---- Choose your default model here ----\n",
    "# If the previous cell set `selected_model`, use it; otherwise default to \"heavy\".\n",
    "try:\n",
    "    MODEL_VARIANT = selected_model.lower().strip()\n",
    "except NameError:\n",
    "    MODEL_VARIANT = \"heavy\"   # default\n",
    "\n",
    "MODEL_PATH = download_pose_model(MODEL_VARIANT)\n",
    "\n",
    "# ---- Verify we can initialize the Pose Landmarker (VIDEO mode) ----\n",
    "BaseOptions = mp_python.BaseOptions\n",
    "PoseLandmarker = mp_vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp_vision.PoseLandmarkerOptions\n",
    "RunningMode = mp_vision.RunningMode\n",
    "\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=MODEL_PATH),\n",
    "    running_mode=RunningMode.VIDEO,\n",
    "    num_poses=1,\n",
    "    min_pose_detection_confidence=0.5,\n",
    "    min_pose_presence_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    output_segmentation_masks=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "        print(\"‚úÖ PoseLandmarker initialized successfully (VIDEO mode).\")\n",
    "        print(f\"   Model: {MODEL_VARIANT} ‚Üí {MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to initialize PoseLandmarker. Check the model file and MediaPipe version.\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb2c7cc-0137-4ded-83d2-5879407a66d8",
   "metadata": {},
   "source": [
    "## 4. VIDEO mode: timestamps & inference loop\n",
    "\n",
    "For offline videos, we must use RunningMode.VIDEO and pass a monotonic timestamp (ms) for each frame:\n",
    "\n",
    "* We read frames with OpenCV, compute timestamp_ms = int((frame_idx / fps) * 1000), and call\n",
    "landmarker.detect_for_video(mp_image, timestamp_ms).\n",
    "\n",
    "* The Task returns normalized 2D landmarks (x, y ‚àà [0,1], z depth-like, plus visibility) and world 3D landmarks (x_m, y_m, z_m in meters).\n",
    "\n",
    "* We‚Äôll save tidy CSV files for 2D and 3D landmarks.\n",
    "\n",
    "* We‚Äôll also write an annotated MP4 by drawing a simple skeleton over each frame.\n",
    "\n",
    "#### Parameters you can tune\n",
    "\n",
    "* `MODEL_VARIANT` (lite/full/heavy), `num_poses` (usually 1), `frame_stride` (skip frames for speed),\n",
    "\n",
    "* `min_pose_detection_confidence`, `min_pose_presence_confidence`, `min_tracking_confidence`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c10b5c6-459c-4e66-b185-beea783fe65b",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Choose Your Parameters\n",
    "\n",
    "Before running extraction, set the **tunable parameters** in the next cell.  \n",
    "These control **model accuracy**, **processing speed**, **output organization**, and **post-processing filters** (anti-jitter smoothing).\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Variant**\n",
    "\n",
    "* **`MODEL_VARIANT`** ‚Äî choose one of:\n",
    "  * `lite` ‚Üí fastest but least accurate  \n",
    "  * `full` ‚Üí balanced (medium accuracy & speed)  \n",
    "  * `heavy` ‚Üí **most accurate** *(default; recommended for LS100 on modern hardware)*\n",
    "\n",
    "> Changing `MODEL_VARIANT` automatically downloads the correct `.task` file to your local `models/` folder if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Inference Settings**\n",
    "\n",
    "* **`frame_stride`** ‚Äî process every *k*-th frame  \n",
    "  * `1` = every frame (maximum precision)  \n",
    "  * `2` = every other frame (faster)  \n",
    "  * `3+` = skip more frames (fastest, least temporal detail)\n",
    "\n",
    "* **`num_poses`** ‚Äî number of people to detect per frame  \n",
    "  * Use `1` for single-person videos (default in LS100)\n",
    "\n",
    "* **Confidence thresholds**  \n",
    "  * `min_pose_detection_confidence` ‚Äî confidence for detecting a pose  \n",
    "  * `min_pose_presence_confidence` ‚Äî confidence that a person is visible  \n",
    "  * `min_tracking_confidence` ‚Äî confidence for stable tracking across frames\n",
    "\n",
    "---\n",
    "\n",
    "### **Output Settings**\n",
    "\n",
    "* **`make_annotated_video`** ‚Äî if `True`, saves an annotated `.mp4` showing the skeleton overlay.  \n",
    "* **`outputs_subdir_name`** ‚Äî defines where outputs are saved:\n",
    "  * All **CSVs** and optional **annotated MP4s** are written to an `outputs/` folder placed **next to each input video** (same directory).\n",
    "\n",
    "---\n",
    "\n",
    "### **Post-Processing Filters (Anti-Jitter)**\n",
    "\n",
    "After landmark extraction, you can smooth or clean the data:\n",
    "\n",
    "* **`visibility_thresh`** ‚Äî discard landmarks with confidence below threshold  \n",
    "* **`hampel_window`** / **`hampel_nsigmas`** ‚Äî outlier removal using a Hampel filter  \n",
    "  * Removes sudden jumps and replaces them with local medians  \n",
    "* **`rolling_window`** ‚Äî rolling average smoother (reduces frame-to-frame jitter)\n",
    "\n",
    "> üí° **Tip:**  \n",
    "> - If you have a slow computer, yo can choose `MODEL_VARIANT = \"lite\"` or `frame_stride = 2` to reduce load.  \n",
    "> - After extraction, apply filtering to clean up the 2D CSV before using it in analysis.\n",
    "\n",
    "---\n",
    "\n",
    "*The Pose Landmarker returns:*\n",
    "- **2D normalized landmarks**: `(x, y ‚àà [0,1])`, `z` (depth-like, unitless), `visibility` (0‚Äì1 confidence).  \n",
    "- **3D world landmarks**: `(x_m, y_m, z_m)` in meters.  \n",
    "\n",
    "Outputs:\n",
    "- **CSV files** for both 2D and 3D landmarks.  \n",
    "- Optional annotated **MP4** with the skeleton overlay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7cc04-d179-4ef8-9f71-b52c61dff4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Parameter Summary =====\n",
      "MODEL_VARIANT                  : heavy\n",
      "MODEL_PATH                     : models/pose_landmarker_heavy.task\n",
      "frame_stride                   : 1\n",
      "num_poses                      : 1\n",
      "confidences (detect,pres,track): 0.5, 0.5, 0.5\n",
      "outputs_subdir_name            : outputs\n",
      "make_annotated_video           : True\n",
      "enable_filtering               : False\n",
      "   ‚Ü≥ Filtering is OFF (fast mode: no smoothing or visibility threshold applied)\n",
      "=============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 5. Parameters ‚Äî YOU CAN EDIT THIS BLOCK\n",
    "# =========================================\n",
    "\n",
    "# --- Model choice ---\n",
    "MODEL_VARIANT = \"heavy\"          # options: \"lite\", \"full\", \"heavy\"\n",
    "\n",
    "# --- Inference behavior ---\n",
    "frame_stride = 1                 # 1=every frame; 2=every other; 3=every third, etc.\n",
    "num_poses = 1                    # typically 1 for single-person videos\n",
    "\n",
    "# Confidence thresholds\n",
    "min_pose_detection_confidence = 0.5\n",
    "min_pose_presence_confidence  = 0.5\n",
    "min_tracking_confidence       = 0.5\n",
    "\n",
    "# --- Output location ---\n",
    "# If you input a single video file ‚Üí outputs will be saved to: <video_dir>/<outputs_subdir_name>/\n",
    "# If you input a folder path ‚Üí outputs will be saved to: <parent_of_folder>/<outputs_subdir_name>/\n",
    "outputs_subdir_name  = \"outputs\"\n",
    "make_annotated_video = True      # set False to skip saving annotated MP4s\n",
    "\n",
    "# --- Post-processing filters (applied AFTER extraction to the 2D CSV) ---\n",
    "# NOTE: Filtering improves smoothness but is slower. Turn off to speed up runs.\n",
    "enable_filtering  = True        # ‚Üê students toggle this (True/False)\n",
    "visibility_thresh = 0.5          # keep rows where visibility >= this\n",
    "hampel_window     = 7            # odd int (in frames); robust outlier window\n",
    "hampel_nsigmas    = 3.0          # sensitivity for Hampel (higher = fewer outliers)\n",
    "rolling_window    = 3            # odd int (in frames); centered rolling average for x,y\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 5.a DO NOT EDIT BELOW ‚Äî this ensures everything runs correctly\n",
    "# ======================================================\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Ensure filtering utility is available (only defines if missing) ---\n",
    "try:\n",
    "    apply_filters_to_pose2d\n",
    "except NameError:\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "\n",
    "    def _hampel(series: pd.Series, window: int, nsigmas: float) -> pd.Series:\n",
    "        med = series.rolling(window, center=True, min_periods=1).median()\n",
    "        diff = (series - med).abs()\n",
    "        mad  = diff.rolling(window, center=True, min_periods=1).median()\n",
    "        thr  = nsigmas * 1.4826 * mad.fillna(0)\n",
    "        outlier = diff > thr\n",
    "        return series.where(~outlier, med)\n",
    "\n",
    "    def apply_filters_to_pose2d(csv2d_path: str,\n",
    "                                visibility_thresh: float = 0.5,\n",
    "                                rolling_window: int = 3,\n",
    "                                hampel_window: int = 7,\n",
    "                                hampel_nsigmas: float = 3.0) -> str:\n",
    "        \"\"\"\n",
    "        Saves a filtered CSV next to the original as *_filtered.csv.\n",
    "        Returns the filtered path.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv2d_path)\n",
    "        if \"visibility\" in df.columns:\n",
    "            df = df[df[\"visibility\"].fillna(0.0) >= visibility_thresh].copy()\n",
    "\n",
    "        # Sort for stable rolling ops\n",
    "        df = df.sort_values([\"video\",\"landmark_index\",\"frame\"])\n",
    "\n",
    "        # Hampel (robust outlier removal), then rolling mean smooth\n",
    "        for coord in (\"x\",\"y\"):\n",
    "            if coord in df.columns:\n",
    "                df[coord] = (\n",
    "                    df.groupby([\"video\",\"landmark_index\"], group_keys=False)[coord]\n",
    "                      .apply(lambda s: _hampel(s, hampel_window, hampel_nsigmas))\n",
    "                      .rolling(rolling_window, center=True, min_periods=1).mean()\n",
    "                )\n",
    "\n",
    "        out_path = Path(csv2d_path).with_name(Path(csv2d_path).stem + \"_filtered.csv\")\n",
    "        df.to_csv(out_path, index=False)\n",
    "        return str(out_path)\n",
    "\n",
    "# Ensure MODEL_PATH exists (downloaded earlier)\n",
    "try:\n",
    "    MODEL_PATH\n",
    "except NameError:\n",
    "    raise RuntimeError(\"MODEL_PATH not found. Please run the previous model download cell first.\")\n",
    "\n",
    "# --- Helper: Ensure odd window sizes for filters ---\n",
    "def _ensure_odd(n: int) -> int:\n",
    "    try:\n",
    "        n = int(n)\n",
    "    except Exception:\n",
    "        return 3\n",
    "    return n if n % 2 == 1 else n + 1\n",
    "\n",
    "hampel_window  = _ensure_odd(hampel_window)\n",
    "rolling_window = _ensure_odd(rolling_window)\n",
    "\n",
    "# --- Helper: Resolve output folder location ---\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\", \".m4v\", \".avi\", \".mkv\"}\n",
    "\n",
    "def resolve_outputs_dir(input_path: str | Path, outputs_subdir_name: str = \"outputs\") -> Path:\n",
    "    \"\"\"\n",
    "    If input is a file (has a known video extension):\n",
    "        -> <file_dir>/<outputs_subdir_name>/\n",
    "    If input is a folder (no extension):\n",
    "        -> <parent_of_folder>/<outputs_subdir_name>/\n",
    "    \"\"\"\n",
    "    p = Path(input_path)\n",
    "    if p.is_file() or p.suffix.lower() in VIDEO_EXTS:\n",
    "        return p.parent / outputs_subdir_name\n",
    "    else:\n",
    "        return p.parent / outputs_subdir_name\n",
    "\n",
    "# --- Sanity check summary ---\n",
    "print(\"\\n===== Parameter Summary =====\")\n",
    "print(f\"MODEL_VARIANT                  : {MODEL_VARIANT}\")\n",
    "print(f\"MODEL_PATH                     : {MODEL_PATH}\")\n",
    "print(f\"frame_stride                   : {frame_stride}\")\n",
    "print(f\"num_poses                      : {num_poses}\")\n",
    "print(f\"confidences (detect,pres,track): {min_pose_detection_confidence}, \"\n",
    "      f\"{min_pose_presence_confidence}, {min_tracking_confidence}\")\n",
    "print(f\"outputs_subdir_name            : {outputs_subdir_name}\")\n",
    "print(f\"make_annotated_video           : {make_annotated_video}\")\n",
    "print(f\"enable_filtering               : {enable_filtering}\")\n",
    "if enable_filtering:\n",
    "    print(f\"   visibility_thresh           : {visibility_thresh}\")\n",
    "    print(f\"   hampel_window / nsigmas     : {hampel_window} / {hampel_nsigmas}\")\n",
    "    print(f\"   rolling_window              : {rolling_window}\")\n",
    "else:\n",
    "    print(\"   ‚Ü≥ Filtering is OFF (fast mode: no smoothing or visibility threshold applied)\")\n",
    "print(\"=============================\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3f1b8b8-5c7a-4c9e-83d3-a99de9e7ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 6. Function: Extract pose landmarks from a video file\n",
    "#    - Writes RAW outputs according to the new folder rules\n",
    "#    - Filtering (if enabled) happens in the NEXT block\n",
    "# =========================================================\n",
    "\n",
    "# Reuse landmark names if already defined; else define here.\n",
    "try:\n",
    "    landmark_index_to_name\n",
    "except NameError:\n",
    "    POSE_LANDMARK_NAMES = [\n",
    "        \"nose\",\"left_eye_inner\",\"left_eye\",\"left_eye_outer\",\n",
    "        \"right_eye_inner\",\"right_eye\",\"right_eye_outer\",\n",
    "        \"left_ear\",\"right_ear\",\"mouth_left\",\"mouth_right\",\n",
    "        \"left_shoulder\",\"right_shoulder\",\"left_elbow\",\"right_elbow\",\n",
    "        \"left_wrist\",\"right_wrist\",\"left_pinky\",\"right_pinky\",\n",
    "        \"left_index\",\"right_index\",\"left_thumb\",\"right_thumb\",\n",
    "        \"left_hip\",\"right_hip\",\"left_knee\",\"right_knee\",\n",
    "        \"left_ankle\",\"right_ankle\",\"left_heel\",\"right_heel\",\n",
    "        \"left_foot_index\",\"right_foot_index\",\n",
    "    ]\n",
    "    landmark_index_to_name = {i: n for i, n in enumerate(POSE_LANDMARK_NAMES)}\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Dict\n",
    "\n",
    "def extract_pose_from_video(\n",
    "    video_path: Union[str, Path],\n",
    "    model_path: Union[str, Path],\n",
    "    make_annotated_video: bool = False,\n",
    "    frame_stride: int = 1,\n",
    "    num_poses: int = 1,\n",
    "    min_pose_detection_confidence: float = 0.5,\n",
    "    min_pose_presence_confidence: float = 0.5,\n",
    "    min_tracking_confidence: float = 0.5,\n",
    "    output_segmentation_masks: bool = False,\n",
    "    # If provided, write outputs here; else follow file/folder rules via resolve_outputs_dir(...)\n",
    "    base_outputs_dir: Optional[Union[str, Path]] = None,\n",
    ") -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extracts pose landmarks from a single video and saves:\n",
    "      - 2D CSV (RAW):   <outputs>/<video_stem>_pose2d.csv\n",
    "      - 3D CSV (RAW):   <outputs>/<video_stem>_pose3d.csv  (if world landmarks available)\n",
    "      - MP4 (optional): <outputs>/<video_stem>_annotated.mp4\n",
    "\n",
    "    Output folder rules:\n",
    "      ‚Ä¢ If base_outputs_dir is given ‚Üí use it.\n",
    "      ‚Ä¢ Else (single-file default)   ‚Üí <video_dir>/<outputs_subdir_name>/\n",
    "        (outputs_subdir_name is set in the Parameters cell).\n",
    "\n",
    "    NOTE: Any smoothing/jitter filtering is performed in the NEXT block.\n",
    "    \"\"\"\n",
    "    import cv2, numpy as np, pandas as pd\n",
    "    import mediapipe as mp\n",
    "    from mediapipe.tasks import python as mp_python\n",
    "    from mediapipe.tasks.python import vision as mp_vision\n",
    "\n",
    "    video_path = Path(video_path)\n",
    "    model_path = str(model_path)\n",
    "\n",
    "    # Determine output directory\n",
    "    if base_outputs_dir is not None:\n",
    "        out_dir = Path(base_outputs_dir)\n",
    "    else:\n",
    "        try:\n",
    "            out_dir = resolve_outputs_dir(video_path, outputs_subdir_name=outputs_subdir_name)\n",
    "        except NameError:\n",
    "            out_dir = video_path.parent / (outputs_subdir_name if 'outputs_subdir_name' in globals() else 'outputs')\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    stem = video_path.stem\n",
    "    csv2d   = out_dir / f\"{stem}_pose2d.csv\"\n",
    "    csv3d   = out_dir / f\"{stem}_pose3d.csv\"\n",
    "    mp4_out = out_dir / f\"{stem}_annotated.mp4\"\n",
    "\n",
    "    # Optional: echo filter toggle (if defined) for clarity\n",
    "    if 'enable_filtering' in globals():\n",
    "        print(f\"[extract] enable_filtering = {enable_filtering} (filtering runs after extraction)\")\n",
    "\n",
    "    # --- OpenCV video IO ---\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise FileNotFoundError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if not fps or fps <= 1e-6:\n",
    "        fps = 30.0  # safe fallback\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    writer = None\n",
    "    if make_annotated_video:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        writer = cv2.VideoWriter(str(mp4_out), fourcc, fps / max(1, frame_stride), (width, height))\n",
    "\n",
    "    # --- MediaPipe Tasks (VIDEO mode) ---\n",
    "    BaseOptions = mp_python.BaseOptions\n",
    "    PoseLandmarker = mp_vision.PoseLandmarker\n",
    "    PoseLandmarkerOptions = mp_vision.PoseLandmarkerOptions\n",
    "    RunningMode = mp_vision.RunningMode\n",
    "\n",
    "    options = PoseLandmarkerOptions(\n",
    "        base_options=BaseOptions(model_asset_path=model_path),\n",
    "        running_mode=RunningMode.VIDEO,\n",
    "        num_poses=num_poses,\n",
    "        min_pose_detection_confidence=min_pose_detection_confidence,\n",
    "        min_pose_presence_confidence=min_pose_presence_confidence,\n",
    "        min_tracking_confidence=min_tracking_confidence,\n",
    "        output_segmentation_masks=output_segmentation_masks,\n",
    "    )\n",
    "\n",
    "    # --- Helpers (image conversion + simple skeleton overlay) ---\n",
    "    def _mp_image_from_bgr(bgr):\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        return mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n",
    "\n",
    "    def _draw_skeleton(bgr, norm_landmarks, visibility_thresh: float = 0.5):\n",
    "        h, w = bgr.shape[:2]\n",
    "        pts = {}\n",
    "        for i, lm in enumerate(norm_landmarks):\n",
    "            vis = getattr(lm, \"visibility\", 1.0) or 0.0\n",
    "            if vis >= visibility_thresh:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                pts[i] = (x, y)\n",
    "                cv2.circle(bgr, (x, y), 2, (255, 255, 255), -1)\n",
    "        for a, b in [\n",
    "            (11,13),(13,15),(12,14),(14,16),(11,12),(23,24),(11,23),(12,24),\n",
    "            (23,25),(25,27),(24,26),(26,28),(27,29),(29,31),(28,30),(30,32)\n",
    "        ]:\n",
    "            if a in pts and b in pts:\n",
    "                cv2.line(bgr, pts[a], pts[b], (255, 255, 255), 2)\n",
    "\n",
    "    rows2d, rows3d = [], []\n",
    "\n",
    "    with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "        frame_idx = 0\n",
    "        while True:\n",
    "            ok, bgr = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "\n",
    "            # Frame skipping for speed\n",
    "            if frame_stride > 1 and (frame_idx % frame_stride != 0):\n",
    "                frame_idx += 1\n",
    "                continue\n",
    "\n",
    "            # VIDEO mode requires monotonic ms timestamps\n",
    "            ts_ms = int((frame_idx / fps) * 1000.0)\n",
    "            mp_image = _mp_image_from_bgr(bgr)\n",
    "            result = landmarker.detect_for_video(mp_image, ts_ms)\n",
    "\n",
    "            for pose_id, nlands in enumerate(result.pose_landmarks):\n",
    "                # 2D normalized landmarks (+ visibility)\n",
    "                for li, lm in enumerate(nlands):\n",
    "                    rows2d.append({\n",
    "                        \"video\": video_path.name,\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"time_ms\": ts_ms,\n",
    "                        \"landmark_index\": li,\n",
    "                        \"landmark_name\": landmark_index_to_name.get(li, str(li)),\n",
    "                        \"x\": lm.x, \"y\": lm.y, \"z\": lm.z,\n",
    "                        \"visibility\": getattr(lm, \"visibility\", np.nan),\n",
    "                    })\n",
    "\n",
    "                # 3D world landmarks (meters), if available\n",
    "                if len(result.pose_world_landmarks) > pose_id:\n",
    "                    wlands = result.pose_world_landmarks[pose_id]\n",
    "                    for li, lm in enumerate(wlands):\n",
    "                        rows3d.append({\n",
    "                            \"video\": video_path.name,\n",
    "                            \"frame\": frame_idx,\n",
    "                            \"time_ms\": ts_ms,\n",
    "                            \"landmark_index\": li,\n",
    "                            \"landmark_name\": landmark_index_to_name.get(li, str(li)),\n",
    "                            \"x_m\": lm.x, \"y_m\": lm.y, \"z_m\": lm.z,\n",
    "                            \"visibility\": getattr(lm, \"visibility\", np.nan),\n",
    "                        })\n",
    "\n",
    "                # Optional overlay\n",
    "                if writer is not None and len(nlands) > 0:\n",
    "                    bgr_draw = bgr.copy()\n",
    "                    _draw_skeleton(bgr_draw, nlands, visibility_thresh=0.5)\n",
    "                    writer.write(bgr_draw)\n",
    "\n",
    "            frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "\n",
    "    # --- Save RAW CSVs ---\n",
    "    import pandas as pd\n",
    "    pd.DataFrame(rows2d).to_csv(csv2d, index=False)\n",
    "    if rows3d:\n",
    "        pd.DataFrame(rows3d).to_csv(csv3d, index=False)\n",
    "        csv3d_str = str(csv3d)\n",
    "    else:\n",
    "        csv3d_str = None\n",
    "\n",
    "    # Return RAW paths; the next block may also produce a *_filtered.csv\n",
    "    return {\n",
    "        \"csv2d\": str(csv2d),\n",
    "        \"csv3d\": csv3d_str,\n",
    "        \"annotated_mp4\": (str(mp4_out) if make_annotated_video else None),\n",
    "    }\n",
    "\n",
    "# --- Quick peek helper (unchanged) ---\n",
    "def peek_csv(path, n=5):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"{path} ‚Üí shape={df.shape}\")\n",
    "    display(df.head(n))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb64ec6c",
   "metadata": {},
   "source": [
    "## Input the path of the video or the folder containing videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7386227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 7. Paste your input path (file OR folder)\n",
    "# =========================================\n",
    "# Examples:\n",
    "# input_path = \"/path/to/video.mp4\"\n",
    "# input_path = \"/path/to/folder_with_videos\"\n",
    "\n",
    "input_path = \"/Users/souvikmandal/Documents/06_Teaching_Mentoring/LS100_comp_etho/2025/media/video/Kevin_2022_Day5_CRNCH.mp4\"  # ‚Üê paste here (keep quotes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e5b9e",
   "metadata": {},
   "source": [
    "## Now, run the coe blcok below.\n",
    "\n",
    "Pleae note that if you set the enable_filtering (the jitter filter) to `True`, it will take loger time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45793be1-7578-42b3-af37-52a96a9a585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single video detected.\n",
      "Outputs will be saved to: /Users/souvikmandal/Documents/06_Teaching_Mentoring/LS100_comp_etho/2025/media/video/outputs\n",
      "[extract] enable_filtering = False (filtering runs after extraction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1761594301.579539 53872720 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M2 Max\n",
      "W0000 00:00:1761594301.656664 54157257 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761594301.732664 54157256 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úî Done.\n",
      "2D CSV : /Users/souvikmandal/Documents/06_Teaching_Mentoring/LS100_comp_etho/2025/media/video/outputs/Kevin_2022_Day5_CRNCH_pose2d.csv\n",
      "3D CSV : /Users/souvikmandal/Documents/06_Teaching_Mentoring/LS100_comp_etho/2025/media/video/outputs/Kevin_2022_Day5_CRNCH_pose3d.csv\n",
      "MP4    : /Users/souvikmandal/Documents/06_Teaching_Mentoring/LS100_comp_etho/2025/media/video/outputs/Kevin_2022_Day5_CRNCH_annotated.mp4\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 8. Run extraction and save outputs (CSV + annotated MP4s)\n",
    "#    - If input_path is a single video file: outputs ‚Üí <video_dir>/<outputs_subdir_name>/\n",
    "#    - If input_path is a folder: outputs (shared) ‚Üí <parent_of_folder>/<outputs_subdir_name>/\n",
    "#    - Writes a manifest CSV when processing a folder\n",
    "# =========================================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "if not input_path or not str(input_path).strip():\n",
    "    raise ValueError(\"Please set `input_path` in the previous cell.\")\n",
    "\n",
    "p = Path(input_path).expanduser().resolve()\n",
    "\n",
    "# Ensure resolver exists (it was defined in ¬ß5.a)\n",
    "try:\n",
    "    resolve_outputs_dir\n",
    "except NameError:\n",
    "    # Minimal fallback (same logic as earlier)\n",
    "    VIDEO_EXTS = {\".mp4\", \".mov\", \".m4v\", \".avi\", \".mkv\"}\n",
    "    def resolve_outputs_dir(input_path, outputs_subdir_name=\"outputs\"):\n",
    "        ip = Path(input_path)\n",
    "        if ip.is_file() or ip.suffix.lower() in VIDEO_EXTS:\n",
    "            return ip.parent / outputs_subdir_name\n",
    "        else:\n",
    "            return ip.parent / outputs_subdir_name\n",
    "\n",
    "VIDEO_EXTS = VIDEO_EXTS if \"VIDEO_EXTS\" in globals() else {\".mp4\", \".mov\", \".m4v\", \".avi\", \".mkv\"}\n",
    "\n",
    "def _is_video_file(path: Path) -> bool:\n",
    "    return path.is_file() and path.suffix.lower() in VIDEO_EXTS\n",
    "\n",
    "if _is_video_file(p):\n",
    "    # -------- Single video mode --------\n",
    "    base_out = resolve_outputs_dir(p, outputs_subdir_name)\n",
    "    base_out.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Single video detected.\\nOutputs will be saved to: {base_out}\")\n",
    "\n",
    "    outs = extract_pose_from_video(\n",
    "        video_path=str(p),\n",
    "        model_path=MODEL_PATH,\n",
    "        make_annotated_video=make_annotated_video,\n",
    "        frame_stride=frame_stride,\n",
    "        num_poses=num_poses,\n",
    "        min_pose_detection_confidence=min_pose_detection_confidence,\n",
    "        min_pose_presence_confidence=min_pose_presence_confidence,\n",
    "        min_tracking_confidence=min_tracking_confidence,\n",
    "        base_outputs_dir=base_out,  # important\n",
    "    )\n",
    "    print(\"\\n‚úî Done.\")\n",
    "    print(\"2D CSV :\", outs.get(\"csv2d\"))\n",
    "    print(\"3D CSV :\", outs.get(\"csv3d\"))\n",
    "    print(\"MP4    :\", outs.get(\"annotated_mp4\"))\n",
    "\n",
    "else:\n",
    "    # -------- Folder mode --------\n",
    "    if not p.exists() or not p.is_dir():\n",
    "        raise NotADirectoryError(f\"Not a directory: {p}\")\n",
    "\n",
    "    # Shared outputs placed alongside the folder\n",
    "    base_out = resolve_outputs_dir(p, outputs_subdir_name)\n",
    "    base_out.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Folder detected. Outputs will be saved to: {base_out}\")\n",
    "\n",
    "    # Find videos (non-recursive by default; flip to rglob for recursive)\n",
    "    videos = sorted([str(f) for f in p.iterdir() if _is_video_file(f)])\n",
    "    if not videos:\n",
    "        # Try recursive as a helpful fallback\n",
    "        videos = sorted([str(f) for f in p.rglob(\"*\") if _is_video_file(f)])\n",
    "        if videos:\n",
    "            print(f\"Found {len(videos)} video(s) (recursive search).\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No supported video files found in: {p}\")\n",
    "\n",
    "    records = []\n",
    "    for i, vp in enumerate(videos, 1):\n",
    "        print(f\"[{i}/{len(videos)}] {vp}\")\n",
    "        try:\n",
    "            outs = extract_pose_from_video(\n",
    "                video_path=vp,\n",
    "                model_path=MODEL_PATH,\n",
    "                make_annotated_video=make_annotated_video,\n",
    "                frame_stride=frame_stride,\n",
    "                num_poses=num_poses,\n",
    "                min_pose_detection_confidence=min_pose_detection_confidence,\n",
    "                min_pose_presence_confidence=min_pose_presence_confidence,\n",
    "                min_tracking_confidence=min_tracking_confidence,\n",
    "                base_outputs_dir=base_out,  # important\n",
    "            )\n",
    "            records.append({\"video\": vp, **outs, \"status\": \"ok\", \"error\": \"\"})\n",
    "        except Exception as e:\n",
    "            records.append({\"video\": vp, \"csv2d\": None, \"csv3d\": None,\n",
    "                            \"annotated_mp4\": None, \"status\": \"error\", \"error\": str(e)})\n",
    "\n",
    "    manifest = base_out / \"outputs_manifest.csv\"\n",
    "    pd.DataFrame.from_records(records).to_csv(manifest, index=False)\n",
    "    print(f\"\\n‚úî Batch complete. Manifest saved to: {manifest}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e10f64",
   "metadata": {},
   "source": [
    "\n",
    "> **Notes**\n",
    "> - **2D normalized coordinates**: `x,y‚àà[0,1]` relative to image width/height (values can be outside the range if the estimated point is out-of-frame). `z` is depthlike (negative is closer).\n",
    "> - **3D world coordinates**: `x,y,z` are in **meters** in a world coordinate space centered near the hips.  \n",
    "> - **visibility**: confidence for each landmark‚Äôs presence in the frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b2a75b",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Optional: compute simple joint angles\n",
    "\n",
    "#### We will explore this in he next guide\n",
    "Once you have landmarks, you can compute feature engineering targets like **elbow** or **knee angles**. Below is a tiny utility to compute an angle between three named landmarks per frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _angle_between(a, b, c):\n",
    "    # a,b,c are 2D points (x,y) or 3D (x,y,z) ‚Äî here we'll use 2D image coords\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosang = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-9)\n",
    "    cosang = np.clip(cosang, -1.0, 1.0)\n",
    "    return np.degrees(np.arccos(cosang))\n",
    "\n",
    "def compute_joint_angle_csv(csv2d_path: str, joint=(\"left_shoulder\",\"left_elbow\",\"left_wrist\")) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv2d_path)\n",
    "    # wide pivot: columns like x_left_shoulder, y_left_shoulder, etc.\n",
    "    wide = df.pivot_table(index=[\"video\",\"frame\",\"time_ms\"], columns=\"landmark_name\", values=[\"x\",\"y\"])\n",
    "    # helper to get a point\n",
    "    def P(name):\n",
    "        return np.c_[wide[\"x\"][name].values, wide[\"y\"][name].values]\n",
    "    A,B,C = P(joint[0]), P(joint[1]), P(joint[2])\n",
    "    angles = np.array([_angle_between(a,b,c) for a,b,c in zip(A,B,C)])\n",
    "    out = pd.DataFrame({\n",
    "        \"video\": wide.index.get_level_values(\"video\"),\n",
    "        \"frame\": wide.index.get_level_values(\"frame\"),\n",
    "        \"time_ms\": wide.index.get_level_values(\"time_ms\"),\n",
    "        f\"angle_{'_'.join(joint)}\": angles\n",
    "    })\n",
    "    return out\n",
    "\n",
    "# Example (after extraction):\n",
    "# angle_df = compute_joint_angle_csv(\"outputs/yourvideo_pose2d.csv\", (\"left_shoulder\",\"left_elbow\",\"left_wrist\"))\n",
    "# angle_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b8f43",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Notes & best practices\n",
    "\n",
    "- **Timestamps matter:** in `VIDEO` mode you *must* pass `timestamp_ms` that increases with frames; we compute it from frame index and FPS.  \n",
    "- **Tracking saves compute:** in `VIDEO`/`LIVE_STREAM` the task performs pose tracking so the full model isn‚Äôt re-run every frame (helps latency).  \n",
    "- **Out-of-frame landmarks:** 2D normalized `x,y` can be outside `[0,1]` if a joint is off‚Äëscreen; use `visibility` to filter.  \n",
    "- **Model choice:** start with **full**, switch to **lite** for underpowered laptops or large batches, use **heavy** when you need the highest accuracy and can afford the speed.  \n",
    "- **Stride:** a cheap speedup is `frame_stride=2` (¬Ω the frames) or higher.  \n",
    "- **Ethics & consent:** if students process videos of people, teach consent, privacy, and secure storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab198ad",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Troubleshooting\n",
    "- If you see `NoneType` for results, ensure the **model path exists** and your video actually contains a person.  \n",
    "- If you get slowdowns or memory pressure, try `frame_stride=2` or the `\"lite\"` model.  \n",
    "- On some platforms OpenCV MP4 writing may need codecs; if a saved video is empty, try a different `fourcc` (e.g., `cv2.VideoWriter_fourcc(*\"avc1\")`) or install `opencv-python-headless` alternatives.\n",
    "\n",
    "Happy exploring!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MediaPipeEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
