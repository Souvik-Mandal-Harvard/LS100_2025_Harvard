{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbabded9-cf8c-4833-a4be-aab5cb6f98af",
   "metadata": {},
   "source": [
    "# Pose Landmarks with MediaPipe ‚Äî From Local Videos & Folders Using Python\n",
    "\n",
    "This notebook is both a **guided lesson** and a **working pipeline** for detecting human pose landmarks from **local video files** or **entire folders** of videos using **MediaPipe Tasks**.\n",
    "\n",
    "## Goal\n",
    "\n",
    "1. Set up a clean Python 3.12 environment and verify required packages.\n",
    "2. Understand each step and terminologies.\n",
    "3. Download and select a Pose Landmarker model (**lite / full / heavy**) and understand accuracy‚Äìspeed trade-offs.\n",
    "4. Read videos with OpenCV and run inference in **`RunningMode.VIDEO`** with correct **timestamps**.\n",
    "5. Export results as tidy CSVs for analysis: **2D image-normalized** and **3D world** landmarks.\n",
    "6. Create an **annotated MP4** showing the skeleton overlay.\n",
    "7. Build intuition for **visibility**, **image vs. world coordinates**, and simple feature engineering (e.g., joint angles).\n",
    "\n",
    "> **Built for learning:** Along the way you‚Äôll see short callouts explaining *why* each step exists (e.g., timestamps in VIDEO mode), how coordinate spaces differ, and how to tune speed vs. accuracy.\n",
    "\n",
    "## After completing this guide, you will be able to\n",
    "\n",
    "* Load one video‚Äîor loop through an entire folder‚Äîand extract the coordinates of the landmark bodypoints frame-by-frame.\n",
    "* Save two analysis-ready CSVs per video: one for **2D normalized** landmarks and one for **3D world** coordinates.\n",
    "* Produce an **annotated MP4** with landmarks and connections overlaid.\n",
    "* Explain and adjust **`RunningMode.VIDEO`**, **per-frame timestamps**, **visibility filtering**, **image vs. world coordinates**, and model variants (**lite/full/heavy**).\n",
    "\n",
    "> **Prerequisites**\n",
    ">\n",
    "> * Python **3.12** virtual environment selected as the active Jupyter kernel. In case yo8u need help, please refer to the \"LS100_Guide 3_Introduction to Pose Estimation Using MediaPipe.pdf\" guide.\n",
    "> * Installed packages: `mediapipe opencv-python pandas numpy tqdm matplotlib seaborn`\n",
    "> * One or more local video files (e.g., `.mp4`) to test.\n",
    "\n",
    "> **Ethics & consent**\n",
    ">\n",
    "> * If processing videos of people, obtain consent and store data securely. Avoid uploading sensitive content to third-party services.\n",
    "\n",
    "### References for learners\n",
    "\n",
    "* MediaPipe Pose Landmarker (Python guide): [https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker/python](https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker/python)\n",
    "* Pose Landmarker API: [https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/PoseLandmarker](https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/PoseLandmarker)\n",
    "* Model card (BlazePose GHUM 3D; lite/full/heavy):\n",
    "  [https://storage.googleapis.com/mediapipe-assets/Model%20Card%20BlazePose%20GHUM%203D.pdf](https://storage.googleapis.com/mediapipe-assets/Model%20Card%20BlazePose%20GHUM%203D.pdf)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf62f41-3483-4103-9b1e-51d470dc174d",
   "metadata": {},
   "source": [
    "# 0. Environment Setup and Verification (LS100 Standard)\n",
    "\n",
    "Before running any code, make sure you‚Äôre using the **LS100_PoseEstimation_MP** kernel that was created in your Python 3.12 virtual environment.\n",
    "This section verifies your environment and installs all required packages.\n",
    "\n",
    "---\n",
    "\n",
    "### **What you should already have**\n",
    "\n",
    "‚úÖ Python 3.12 installed\n",
    "\n",
    "‚úÖ Virtual environment activated (`(MediaPipeEnv)` should appear in your terminal)\n",
    "\n",
    "‚úÖ Kernel registered as **LS100_PoseEstimation_MP**\n",
    "\n",
    "If you haven‚Äôt completed those steps, revisit the **LS100_Guide 3_Introduction to Pose Estimation Using MediaPipe.pdf** document.\n",
    "\n",
    "---\n",
    "\n",
    "### **Required packages**\n",
    "\n",
    "This notebook uses the following libraries:\n",
    "\n",
    "* `mediapipe` ‚Äì pose landmark model and API\n",
    "* `opencv-python` ‚Äì video I/O (input/output) and frame conversion\n",
    "* `pandas` & `numpy` ‚Äì data handling and analysis\n",
    "* `tqdm` ‚Äì progress bars for video processing\n",
    "* `matplotlib` & `seaborn` ‚Äì visualization and data inspection\n",
    "\n",
    "Run the next cell to ensure these are installed and to confirm the environment details.\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning focus**\n",
    "\n",
    "* Why virtual environments prevent version conflicts\n",
    "* Why we require **Python 3.12** (MediaPipe Tasks currently supports Python 3.9‚Äì3.12 only)\n",
    "* How each library fits into the MediaPipe Pose pipeline\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222fe87d",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Environment setup\n",
    "\n",
    "> If running locally (VS Code/Jupyter), run the following cell once; it might take about a minute to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64456878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Python version: 3.12.12\n",
      "‚úÖ mediapipe already installed\n",
      "‚¨áÔ∏è Installing opencv-python ...\n",
      "‚úÖ pandas already installed\n",
      "‚úÖ numpy already installed\n",
      "‚úÖ tqdm already installed\n",
      "‚úÖ matplotlib already installed\n",
      "‚úÖ seaborn already installed\n",
      "\n",
      "üì¶ Package versions:\n",
      "mediapipe      : 0.10.21\n",
      "opencv-python  : 4.11.0\n",
      "pandas         : 2.3.3\n",
      "numpy          : 1.26.4\n",
      "matplotlib     : 3.10.7\n",
      "seaborn        : 0.13.2\n",
      "\n",
      "‚úÖ Environment is ready to proceed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 0. Environment Setup and Package Verification\n",
    "# ============================================\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "# ---- 1. Check Python version ----\n",
    "py_version = sys.version_info\n",
    "print(f\"üß† Python version: {py_version.major}.{py_version.minor}.{py_version.micro}\")\n",
    "if py_version < (3, 9) or py_version >= (3, 13):\n",
    "    print(\"‚ö†Ô∏è MediaPipe Tasks officially supports Python 3.9‚Äì3.12.\")\n",
    "    print(\"‚ö†Ô∏è Please switch to Python 3.12 for this notebook (as used in LS100).\")\n",
    "\n",
    "# ---- 2. Define required packages ----\n",
    "required_packages = [\n",
    "    \"mediapipe\",\n",
    "    \"opencv-python\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"tqdm\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "]\n",
    "\n",
    "# ---- 3. Function to check and install ----\n",
    "def install_if_missing(pkg):\n",
    "    \"\"\"\n",
    "    Try importing the package; if not found, install it quietly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        importlib.import_module(pkg.split(\"==\")[0])\n",
    "        print(f\"‚úÖ {pkg} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"‚¨áÔ∏è Installing {pkg} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "# ---- 4. Verify each dependency ----\n",
    "for package in required_packages:\n",
    "    install_if_missing(package)\n",
    "\n",
    "# ---- 5. Print package versions for reproducibility ----\n",
    "import mediapipe as mp\n",
    "import cv2, pandas as pd, numpy as np, tqdm, matplotlib, seaborn\n",
    "\n",
    "print(\"\\nüì¶ Package versions:\")\n",
    "print(f\"mediapipe      : {mp.__version__}\")\n",
    "print(f\"opencv-python  : {cv2.__version__}\")\n",
    "print(f\"pandas         : {pd.__version__}\")\n",
    "print(f\"numpy          : {np.__version__}\")\n",
    "print(f\"matplotlib     : {matplotlib.__version__}\")\n",
    "print(f\"seaborn        : {seaborn.__version__}\")\n",
    "\n",
    "print(\"\\n‚úÖ Environment is ready to proceed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccfbb6",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports & version checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ffc92-aa13-4a39-9a17-6315fc1d1e79",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Imports and Version Verification\n",
    "\n",
    "Now that your environment is ready, let‚Äôs import the main libraries used throughout this notebook.\n",
    "\n",
    "This step helps confirm that:\n",
    "\n",
    "* The correct packages are installed inside your LS100 virtual environment\n",
    "* MediaPipe loads successfully (and we can access its **Tasks API**)\n",
    "* OpenCV, NumPy, and Pandas are working properly\n",
    "\n",
    "If an import fails, it usually means you‚Äôre running the notebook in a different kernel (not the one you registered).\n",
    "You can fix that by selecting **Kernel ‚Üí Change Kernel ‚Üí LS100_PoseEstimation_MP** (or the name you chose).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1f6603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MediaPipe Tasks API imported successfully!\n",
      "\n",
      "mediapipe version : 0.10.21\n",
      "opencv version    : 4.11.0\n",
      "pandas version    : 2.3.3\n",
      "numpy version     : 1.26.4\n",
      "‚öôÔ∏è Running on CPU\n",
      "\n",
      " MediaPipe Tasks API is available:\n",
      "- BaseOptions           : True\n",
      "- PoseLandmarker        : True\n",
      "- PoseLandmarkerOptions : True\n",
      "- RunningMode           : True\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 1. Import Libraries and Verify Versions (fixed for MediaPipe >=0.10)\n",
    "# ======================================\n",
    "\n",
    "import os, cv2, numpy as np, pandas as pd, matplotlib, seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python as mp_python\n",
    "from mediapipe.tasks.python import vision as mp_vision\n",
    "\n",
    "print(\"‚úÖ MediaPipe Tasks API imported successfully!\\n\")\n",
    "print(f\"mediapipe version : {mp.__version__}\")\n",
    "print(f\"opencv version    : {cv2.__version__}\")\n",
    "print(f\"pandas version    : {pd.__version__}\")\n",
    "print(f\"numpy version     : {np.__version__}\")\n",
    "\n",
    "# Optional: check GPU availability\n",
    "backend = \"GPU\" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else \"CPU\"\n",
    "print(f\"‚öôÔ∏è Running on {backend}\")\n",
    "\n",
    "# ---- Smoke test: confirm Tasks API symbols exist ----\n",
    "BaseOptions = mp_python.BaseOptions\n",
    "PoseLandmarker = mp_vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp_vision.PoseLandmarkerOptions\n",
    "RunningMode = mp_vision.RunningMode\n",
    "\n",
    "print(\"\\n MediaPipe Tasks API is available:\")\n",
    "print(f\"- BaseOptions           : {BaseOptions is not None}\")\n",
    "print(f\"- PoseLandmarker        : {PoseLandmarker is not None}\")\n",
    "print(f\"- PoseLandmarkerOptions : {PoseLandmarkerOptions is not None}\")\n",
    "print(f\"- RunningMode           : {RunningMode is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d16a0-d5a3-4a65-ae1e-09995f8c0820",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üí° **Notes**\n",
    "\n",
    "* **Why this matters:** ensures that the environment is truly isolated and reproducible.\n",
    "* **Discussion prompt:** Can you tell *why* we check MediaPipe imports *before* running the pipeline? (to confirm the **Tasks** API is available and working).\n",
    "* **TASK:** Print `mp.__file__` to confirm MediaPipe‚Äôs path. This helps you understand where packages live inside the venv.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca8f9e",
   "metadata": {},
   "source": [
    "\n",
    "## 2. How Pose Landmarker works (what to teach)\n",
    "\n",
    "- **Running modes:** `IMAGE`, `VIDEO`, `LIVE_STREAM`. For offline videos we use **`VIDEO`** and must pass a **timestamp (ms)** for each frame; the task uses **tracking** to avoid re-running the full model on every frame (reduces latency at the same accuracy settings).  \n",
    "- **Outputs:**  \n",
    "  - **2D normalized landmarks** in image coordinates (*x,y in [0,1] relative to width/height; z is a depth-like value; visibility in [0,1]*).  \n",
    "  - **3D world landmarks** (meters, origin near hip center; handy for biomechanical features).  \n",
    "- **Variants:** **lite / full / heavy**. Heavier models = more accurate, slower (see model card).  \n",
    "- **Accuracy vs speed knobs:** `num_poses` (usually 1 for single-person), `min_pose_detection_confidence`, `min_pose_presence_confidence`, `min_tracking_confidence`, and **frame stride** (e.g., analyze every 2nd/3rd frame).\n",
    "\n",
    "> We‚Äôll expose all of these transparently in helper functions below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69078313",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Download a Pose Landmarker model (`.task` bundle)\n",
    "\n",
    "Choose one of: `\"lite\"`, `\"full\"` (default), `\"heavy\"`.  \n",
    "URLs follow Google‚Äôs published pattern; we try `latest/‚Ä¶` first and then fall back to version `1/‚Ä¶`.\n",
    "\n",
    "> You only need to download once; it will be cached under `models/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70bce5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading full model from:\n",
      "  https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/latest/pose_landmarker_full.task\n",
      "‚úî Saved to models/pose_landmarker_full.task (9.40 MB)\n",
      "‚úÖ PoseLandmarker initialized successfully (VIDEO mode).\n",
      "   Model: full ‚Üí models/pose_landmarker_full.task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760626676.917819 39312266 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M2 Max\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1760626676.973406 39440242 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760626676.990551 39440236 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 2. Model Selection & Download\n",
    "# ================================\n",
    "import os\n",
    "import pathlib\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python as mp_python\n",
    "from mediapipe.tasks.python import vision as mp_vision\n",
    "\n",
    "# ---- Where to save models ----\n",
    "MODELS_DIR = pathlib.Path(\"models\")\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Official model URLs (latest, with fallback to v1) ----\n",
    "MODEL_URLS = {\n",
    "    \"lite\": [\n",
    "        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/latest/pose_landmarker_lite.task\",\n",
    "        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task\",\n",
    "    ],\n",
    "    \"full\": [\n",
    "        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/latest/pose_landmarker_full.task\",\n",
    "        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/1/pose_landmarker_full.task\",\n",
    "    ],\n",
    "    \"heavy\": [\n",
    "        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/latest/pose_landmarker_heavy.task\",\n",
    "        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "def download_pose_model(variant: str = \"full\") -> str:\n",
    "    \"\"\"\n",
    "    Download the selected model variant (.task) to MODELS_DIR.\n",
    "    Returns the local file path.\n",
    "    \"\"\"\n",
    "    variant = variant.lower().strip()\n",
    "    assert variant in MODEL_URLS, f\"Unknown variant '{variant}'. Choose: lite, full, heavy.\"\n",
    "\n",
    "    out_path = MODELS_DIR / f\"pose_landmarker_{variant}.task\"\n",
    "    if out_path.exists() and out_path.stat().st_size > 50_000:\n",
    "        print(f\"‚úî Model already present: {out_path}\")\n",
    "        return str(out_path)\n",
    "\n",
    "    last_err = None\n",
    "    for url in MODEL_URLS[variant]:\n",
    "        try:\n",
    "            print(f\"Downloading {variant} model from:\\n  {url}\")\n",
    "            with urllib.request.urlopen(url, timeout=60) as r, open(out_path, \"wb\") as f:\n",
    "                f.write(r.read())\n",
    "            if out_path.stat().st_size <= 50_000:\n",
    "                raise RuntimeError(\"Downloaded file seems too small; trying fallback...\")\n",
    "            print(f\"‚úî Saved to {out_path} ({out_path.stat().st_size/1e6:.2f} MB)\")\n",
    "            return str(out_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚Ä¶ failed: {e}\")\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Could not download model for variant '{variant}'. Last error: {last_err}\")\n",
    "\n",
    "# ---- Choose your default model here ----\n",
    "MODEL_VARIANT = \"full\"   # options: \"lite\", \"full\", \"heavy\"\n",
    "MODEL_PATH = download_pose_model(MODEL_VARIANT)\n",
    "\n",
    "# ---- Verify we can initialize the Pose Landmarker (VIDEO mode) ----\n",
    "BaseOptions = mp_python.BaseOptions\n",
    "PoseLandmarker = mp_vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp_vision.PoseLandmarkerOptions\n",
    "RunningMode = mp_vision.RunningMode\n",
    "\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=MODEL_PATH),\n",
    "    running_mode=RunningMode.VIDEO,\n",
    "    num_poses=1,\n",
    "    min_pose_detection_confidence=0.5,\n",
    "    min_pose_presence_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    output_segmentation_masks=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "        print(\"‚úÖ PoseLandmarker initialized successfully (VIDEO mode).\")\n",
    "        print(f\"   Model: {MODEL_VARIANT} ‚Üí {MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to initialize PoseLandmarker. Check the model file and MediaPipe version.\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb2c7cc-0137-4ded-83d2-5879407a66d8",
   "metadata": {},
   "source": [
    "## 4. VIDEO mode: timestamps & inference loop\n",
    "\n",
    "For offline videos, we must use RunningMode.VIDEO and pass a monotonic timestamp (ms) for each frame:\n",
    "\n",
    "* We read frames with OpenCV, compute timestamp_ms = int((frame_idx / fps) * 1000), and call\n",
    "landmarker.detect_for_video(mp_image, timestamp_ms).\n",
    "\n",
    "* The Task returns normalized 2D landmarks (x, y ‚àà [0,1], z depth-like, plus visibility) and world 3D landmarks (x_m, y_m, z_m in meters).\n",
    "\n",
    "* We‚Äôll save tidy CSV files for 2D and 3D landmarks.\n",
    "\n",
    "* We‚Äôll also write an annotated MP4 by drawing a simple skeleton over each frame.\n",
    "\n",
    "#### Parameters you can tune\n",
    "\n",
    "* `MODEL_VARIANT` (lite/full/heavy), `num_poses` (usually 1), `frame_stride` (skip frames for speed),\n",
    "\n",
    "* `min_pose_detection_confidence`, `min_pose_presence_confidence`, `min_tracking_confidence`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c10b5c6-459c-4e66-b185-beea783fe65b",
   "metadata": {},
   "source": [
    "# 5 Choose Your Parameters\n",
    "\n",
    "Before running extraction, set the **tunable parameters** in the next cell.\n",
    "These variables control the accuracy, speed, and outputs of your pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Parameters**\n",
    "\n",
    "* **`MODEL_VARIANT`** ‚Äî choose one of:\n",
    "\n",
    "  * `lite` ‚Üí fastest but lowest accuracy\n",
    "  * `full` ‚Üí balanced and recommended default\n",
    "  * `heavy` ‚Üí most accurate but slower\n",
    "\n",
    "* **`frame_stride`** ‚Äî process every *k*-th frame.\n",
    "\n",
    "  * `1` = analyze every frame (slow but detailed)\n",
    "  * `2` = every other frame (twice as fast)\n",
    "  * `3+` = even faster but less temporal precision\n",
    "\n",
    "* **`num_poses`** ‚Äî number of people to detect per frame.\n",
    "\n",
    "  * Use `1` for single-person videos (default for LS100).\n",
    "\n",
    "* **Confidence thresholds**\n",
    "\n",
    "  * `min_pose_detection_confidence` ‚Üí how certain a pose must be detected\n",
    "  * `min_pose_presence_confidence` ‚Üí confidence threshold for the person‚Äôs presence\n",
    "  * `min_tracking_confidence` ‚Üí how reliably MediaPipe should track the same pose across frames\n",
    "\n",
    "* **`make_annotated_video`** ‚Äî if `True`, saves a new video (`.mp4`) with skeleton overlays.\n",
    "\n",
    "* **`out_dir`** ‚Äî the directory where all **CSVs** and (optional) **annotated MP4s** will be saved.\n",
    "\n",
    "---\n",
    "\n",
    "> üß† **Tip:** If you change `MODEL_VARIANT`, the model will automatically download the correct `.task` file again.\n",
    "> Choose the settings thoughtfully based on your hardware and analysis needs‚Äîstudents with slower laptops should prefer `lite` and/or increase `frame_stride`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ab7cc04-d179-4ef8-9f71-b52c61dff4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading heavy model from:\n",
      "  https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/latest/pose_landmarker_heavy.task\n",
      "‚úî Saved to models/pose_landmarker_heavy.task (30.66 MB)\n",
      "‚úî Using model: heavy ‚Üí models/pose_landmarker_heavy.task\n",
      "   frame_stride=1, num_poses=1, annotated=True\n",
      "   confidences: detection=0.5, presence=0.5, tracking=0.5\n",
      "   output dir: outputs\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 2.1 Parameters ‚Äî YOU control the pipeline\n",
    "# =========================================\n",
    "\n",
    "# --- Model choice ---\n",
    "MODEL_VARIANT = \"heavy\"          # options: \"lite\", \"full\", \"heavy\"\n",
    "\n",
    "# --- Inference behavior ---\n",
    "frame_stride = 1                 # 1=every frame; 2=every other; 3=every third; etc.\n",
    "num_poses = 1                    # usually 1 for single-person videos\n",
    "min_pose_detection_confidence = 0.5\n",
    "min_pose_presence_confidence  = 0.5\n",
    "min_tracking_confidence       = 0.5\n",
    "\n",
    "# --- Outputs ---\n",
    "make_annotated_video = True      # set False to skip saving annotated MP4\n",
    "out_dir = \"outputs\"              # where CSVs / MP4 will be written\n",
    "\n",
    "# --- Sync the model file if variant changed ---\n",
    "# Assumes the download_pose_model() function from earlier cells is defined.\n",
    "def ensure_model_variant(variant: str) -> str:\n",
    "    v = variant.strip().lower()\n",
    "    if v not in (\"lite\", \"full\", \"heavy\"):\n",
    "        raise ValueError(\"MODEL_VARIANT must be one of: 'lite', 'full', 'heavy'.\")\n",
    "    return download_pose_model(v)\n",
    "\n",
    "MODEL_PATH = ensure_model_variant(MODEL_VARIANT)\n",
    "print(f\"‚úî Using model: {MODEL_VARIANT} ‚Üí {MODEL_PATH}\")\n",
    "print(f\"   frame_stride={frame_stride}, num_poses={num_poses}, annotated={make_annotated_video}\")\n",
    "print(f\"   confidences: detection={min_pose_detection_confidence}, presence={min_pose_presence_confidence}, tracking={min_tracking_confidence}\")\n",
    "print(f\"   output dir: {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3f1b8b8-5c7a-4c9e-83d3-a99de9e7ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extraction function updated to include landmark_name in both CSVs.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 3.1 Ensure CSVs include landmark_name & add peek helpers\n",
    "# =========================================================\n",
    "\n",
    "# If you already defined POSE_LANDMARK_NAMES & landmark_index_to_name earlier, we reuse them.\n",
    "# If not, define them quickly here:\n",
    "try:\n",
    "    landmark_index_to_name\n",
    "except NameError:\n",
    "    POSE_LANDMARK_NAMES = [\n",
    "        \"nose\",\"left_eye_inner\",\"left_eye\",\"left_eye_outer\",\n",
    "        \"right_eye_inner\",\"right_eye\",\"right_eye_outer\",\n",
    "        \"left_ear\",\"right_ear\",\"mouth_left\",\"mouth_right\",\n",
    "        \"left_shoulder\",\"right_shoulder\",\"left_elbow\",\"right_elbow\",\n",
    "        \"left_wrist\",\"right_wrist\",\"left_pinky\",\"right_pinky\",\n",
    "        \"left_index\",\"right_index\",\"left_thumb\",\"right_thumb\",\n",
    "        \"left_hip\",\"right_hip\",\"left_knee\",\"right_knee\",\n",
    "        \"left_ankle\",\"right_ankle\",\"left_heel\",\"right_heel\",\n",
    "        \"left_foot_index\",\"right_foot_index\",\n",
    "    ]\n",
    "    landmark_index_to_name = {i: n for i, n in enumerate(POSE_LANDMARK_NAMES)}\n",
    "\n",
    "# --- Re-define extract_pose_from_video to include `landmark_name` columns ---\n",
    "def extract_pose_from_video(\n",
    "    video_path: str,\n",
    "    model_path: str,\n",
    "    out_dir: str = \"outputs\",\n",
    "    make_annotated_video: bool = False,\n",
    "    frame_stride: int = 1,\n",
    "    num_poses: int = 1,\n",
    "    min_pose_detection_confidence: float = 0.5,\n",
    "    min_pose_presence_confidence: float = 0.5,\n",
    "    min_tracking_confidence: float = 0.5,\n",
    "    output_segmentation_masks: bool = False,\n",
    "):\n",
    "    import os, pathlib\n",
    "    import cv2, numpy as np, pandas as pd\n",
    "    import mediapipe as mp\n",
    "    from mediapipe.tasks import python as mp_python\n",
    "    from mediapipe.tasks.python import vision as mp_vision\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    stem = pathlib.Path(video_path).stem\n",
    "    csv2d = os.path.join(out_dir, f\"{stem}_pose2d.csv\")\n",
    "    csv3d = os.path.join(out_dir, f\"{stem}_pose3d.csv\")\n",
    "    mp4_out = os.path.join(out_dir, f\"{stem}_annotated.mp4\")\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise FileNotFoundError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    writer = None\n",
    "    if make_annotated_video:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        writer  = cv2.VideoWriter(mp4_out, fourcc, fps / max(1, frame_stride), (width, height))\n",
    "\n",
    "    BaseOptions = mp_python.BaseOptions\n",
    "    PoseLandmarker = mp_vision.PoseLandmarker\n",
    "    PoseLandmarkerOptions = mp_vision.PoseLandmarkerOptions\n",
    "    RunningMode = mp_vision.RunningMode\n",
    "\n",
    "    options = PoseLandmarkerOptions(\n",
    "        base_options=BaseOptions(model_asset_path=model_path),\n",
    "        running_mode=RunningMode.VIDEO,\n",
    "        num_poses=num_poses,\n",
    "        min_pose_detection_confidence=min_pose_detection_confidence,\n",
    "        min_pose_presence_confidence=min_pose_presence_confidence,\n",
    "        min_tracking_confidence=min_tracking_confidence,\n",
    "        output_segmentation_masks=output_segmentation_masks,\n",
    "    )\n",
    "\n",
    "    # Minimal overlay helper (same as earlier cell)\n",
    "    def _mp_image_from_bgr(bgr):\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        return mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n",
    "\n",
    "    def _draw_skeleton(bgr, norm_landmarks, visibility_thresh=0.5):\n",
    "        h, w = bgr.shape[:2]\n",
    "        pts = {}\n",
    "        for i, lm in enumerate(norm_landmarks):\n",
    "            vis = getattr(lm, \"visibility\", 1.0) or 0.0\n",
    "            if vis >= visibility_thresh:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                pts[i] = (x, y)\n",
    "                cv2.circle(bgr, (x, y), 2, (255, 255, 255), -1)\n",
    "        for a, b in [\n",
    "            (11,13),(13,15),(12,14),(14,16),(11,12),(23,24),(11,23),(12,24),\n",
    "            (23,25),(25,27),(24,26),(26,28),(27,29),(29,31),(28,30),(30,32)\n",
    "        ]:\n",
    "            if a in pts and b in pts:\n",
    "                cv2.line(bgr, pts[a], pts[b], (255, 255, 255), 2)\n",
    "\n",
    "    rows2d, rows3d = [], []\n",
    "\n",
    "    with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "        frame_idx = 0\n",
    "        while True:\n",
    "            ok, bgr = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "            if frame_stride > 1 and (frame_idx % frame_stride != 0):\n",
    "                frame_idx += 1\n",
    "                continue\n",
    "\n",
    "            ts_ms = int((frame_idx / fps) * 1000.0)\n",
    "            mp_image = _mp_image_from_bgr(bgr)\n",
    "            result = landmarker.detect_for_video(mp_image, ts_ms)\n",
    "\n",
    "            for pose_id, nlands in enumerate(result.pose_landmarks):\n",
    "                # 2D\n",
    "                for li, lm in enumerate(nlands):\n",
    "                    rows2d.append({\n",
    "                        \"video\": os.path.basename(video_path),\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"time_ms\": ts_ms,\n",
    "                        \"landmark_index\": li,\n",
    "                        \"landmark_name\": landmark_index_to_name.get(li, str(li)),\n",
    "                        \"x\": lm.x,\n",
    "                        \"y\": lm.y,\n",
    "                        \"z\": lm.z,\n",
    "                        \"visibility\": getattr(lm, \"visibility\", np.nan),\n",
    "                    })\n",
    "                # 3D\n",
    "                if len(result.pose_world_landmarks) > pose_id:\n",
    "                    wlands = result.pose_world_landmarks[pose_id]\n",
    "                    for li, lm in enumerate(wlands):\n",
    "                        rows3d.append({\n",
    "                            \"video\": os.path.basename(video_path),\n",
    "                            \"frame\": frame_idx,\n",
    "                            \"time_ms\": ts_ms,\n",
    "                            \"landmark_index\": li,\n",
    "                            \"landmark_name\": landmark_index_to_name.get(li, str(li)),\n",
    "                            \"x_m\": lm.x,\n",
    "                            \"y_m\": lm.y,\n",
    "                            \"z_m\": lm.z,\n",
    "                            \"visibility\": getattr(lm, \"visibility\", np.nan),\n",
    "                        })\n",
    "\n",
    "                if writer is not None and len(nlands) > 0:\n",
    "                    bgr_draw = bgr.copy()\n",
    "                    _draw_skeleton(bgr_draw, nlands, visibility_thresh=0.5)\n",
    "                    writer.write(bgr_draw)\n",
    "\n",
    "            frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "\n",
    "    pd.DataFrame(rows2d).to_csv(csv2d, index=False)\n",
    "    if rows3d:\n",
    "        pd.DataFrame(rows3d).to_csv(csv3d, index=False)\n",
    "    else:\n",
    "        csv3d = None\n",
    "\n",
    "    return {\"csv2d\": csv2d, \"csv3d\": csv3d, \"annotated_mp4\": (mp4_out if make_annotated_video else None)}\n",
    "\n",
    "# --- Quick peek helper ---\n",
    "def peek_csv(path, n=5):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"{path} ‚Üí shape={df.shape}\")\n",
    "    display(df.head(n))\n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Extraction function updated to include landmark_name in both CSVs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45793be1-7578-42b3-af37-52a96a9a585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760639515.407024 39312266 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M2 Max\n",
      "W0000 00:00:1760639515.624024 39623274 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760639517.299738 39623273 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'csv2d': 'outputs/sprintblockstart_221008_clip017_isaiah_pose2d.csv',\n",
       " 'csv3d': 'outputs/sprintblockstart_221008_clip017_isaiah_pose3d.csv',\n",
       " 'annotated_mp4': 'outputs/sprintblockstart_221008_clip017_isaiah_annotated.mp4'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Quick usage example (edit the path) ---\n",
    "# Example call using the ‚Äú2.1 Parameters‚Äù variables the student set:\n",
    "outputs = extract_pose_from_video(\n",
    "    video_path=\"/Users/souvikmandal/Documents/06_Teaching_Mentoring/LS100_comp_etho/2022/Celine_blockstart/videos/sprintblockstart_221008_clip017_isaiah.MOV\",\n",
    "    model_path=MODEL_PATH,                # from the model download step\n",
    "    out_dir=out_dir,\n",
    "    make_annotated_video=make_annotated_video,\n",
    "    frame_stride=frame_stride,\n",
    "    num_poses=num_poses,\n",
    "    min_pose_detection_confidence=min_pose_detection_confidence,\n",
    "    min_pose_presence_confidence=min_pose_presence_confidence,\n",
    "    min_tracking_confidence=min_tracking_confidence,\n",
    ")\n",
    "outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e10f64",
   "metadata": {},
   "source": [
    "\n",
    "> **Notes for students**\n",
    "> - **2D normalized coordinates**: `x,y‚àà[0,1]` relative to image width/height (values can be outside the range if the estimated point is out-of-frame). `z` is depthlike (negative is closer).\n",
    "> - **3D world coordinates**: `x,y,z` are in **meters** in a world coordinate space centered near the hips.  \n",
    "> - **visibility**: confidence for each landmark‚Äôs presence in the frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dacf32f",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Batch mode ‚Äî process a directory of videos\n",
    "\n",
    "Set `input_dir` and an optional `glob` pattern (e.g., `\"*.mp4\"`). Each video will\n",
    "produce two CSVs (2D + 3D) and, if enabled, an annotated MP4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58da1caf-5389-4453-8ac7-3b4921cd47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. Batch Processing: process a folder of videos\n",
    "# ============================================\n",
    "import os, glob, pathlib, pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Common video extensions to search for (case-insensitive)\n",
    "VIDEO_EXTS = (\".mp4\", \".mov\", \".m4v\", \".avi\", \".mkv\")\n",
    "\n",
    "def list_videos(input_dir: str, recursive: bool = False) -> List[str]:\n",
    "    \"\"\"Return a sorted list of video file paths in the directory.\"\"\"\n",
    "    p = pathlib.Path(input_dir)\n",
    "    if not p.exists() or not p.is_dir():\n",
    "        raise NotADirectoryError(f\"Not a directory: {input_dir}\")\n",
    "    if recursive:\n",
    "        files = [str(f) for f in p.rglob(\"*\") if f.suffix.lower() in VIDEO_EXTS]\n",
    "    else:\n",
    "        files = [str(f) for f in p.iterdir() if f.is_file() and f.suffix.lower() in VIDEO_EXTS]\n",
    "    return sorted(files)\n",
    "\n",
    "def batch_process_videos(\n",
    "    input_dir: str,\n",
    "    model_path: str,\n",
    "    out_dir: str = \"outputs\",\n",
    "    recursive: bool = False,\n",
    "    # Extraction params (default to Section 2.1 variables if they exist)\n",
    "    make_annotated_video: Optional[bool] = None,\n",
    "    frame_stride: Optional[int] = None,\n",
    "    num_poses: Optional[int] = None,\n",
    "    min_pose_detection_confidence: Optional[float] = None,\n",
    "    min_pose_presence_confidence: Optional[float] = None,\n",
    "    min_tracking_confidence: Optional[float] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all videos in a directory and return a summary table\n",
    "    with paths to the generated CSVs and (optional) annotated MP4s.\n",
    "    \"\"\"\n",
    "    # Pull defaults from the Section 2.1 variables if not provided\n",
    "    globals_fallbacks = {\n",
    "        \"make_annotated_video\": True,\n",
    "        \"frame_stride\": 1,\n",
    "        \"num_poses\": 1,\n",
    "        \"min_pose_detection_confidence\": 0.5,\n",
    "        \"min_pose_presence_confidence\": 0.5,\n",
    "        \"min_tracking_confidence\": 0.5,\n",
    "    }\n",
    "    g = globals()\n",
    "    if make_annotated_video is None: make_annotated_video = g.get(\"make_annotated_video\", globals_fallbacks[\"make_annotated_video\"])\n",
    "    if frame_stride is None: frame_stride = g.get(\"frame_stride\", globals_fallbacks[\"frame_stride\"])\n",
    "    if num_poses is None: num_poses = g.get(\"num_poses\", globals_fallbacks[\"num_poses\"])\n",
    "    if min_pose_detection_confidence is None: min_pose_detection_confidence = g.get(\"min_pose_detection_confidence\", globals_fallbacks[\"min_pose_detection_confidence\"])\n",
    "    if min_pose_presence_confidence is None:  min_pose_presence_confidence  = g.get(\"min_pose_presence_confidence\",  globals_fallbacks[\"min_pose_presence_confidence\"])\n",
    "    if min_tracking_confidence is None:       min_tracking_confidence       = g.get(\"min_tracking_confidence\",       globals_fallbacks[\"min_tracking_confidence\"])\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    files = list_videos(input_dir, recursive=recursive)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No supported video files found in: {input_dir}\")\n",
    "\n",
    "    records: List[Dict[str, Optional[str]]] = []\n",
    "    print(f\"Found {len(files)} video(s) in {input_dir} (recursive={recursive}).\\n\")\n",
    "\n",
    "    for fpath in tqdm(files, desc=\"Batch processing\", unit=\"video\"):\n",
    "        try:\n",
    "            outs = extract_pose_from_video(\n",
    "                video_path=fpath,\n",
    "                model_path=model_path,\n",
    "                out_dir=out_dir,\n",
    "                make_annotated_video=make_annotated_video,\n",
    "                frame_stride=frame_stride,\n",
    "                num_poses=num_poses,\n",
    "                min_pose_detection_confidence=min_pose_detection_confidence,\n",
    "                min_pose_presence_confidence=min_pose_presence_confidence,\n",
    "                min_tracking_confidence=min_tracking_confidence,\n",
    "            )\n",
    "            records.append({\n",
    "                \"video\": fpath,\n",
    "                \"csv2d\": outs.get(\"csv2d\"),\n",
    "                \"csv3d\": outs.get(\"csv3d\"),\n",
    "                \"annotated_mp4\": outs.get(\"annotated_mp4\"),\n",
    "                \"status\": \"ok\",\n",
    "                \"error\": \"\",\n",
    "            })\n",
    "        except Exception as e:\n",
    "            records.append({\n",
    "                \"video\": fpath,\n",
    "                \"csv2d\": None,\n",
    "                \"csv3d\": None,\n",
    "                \"annotated_mp4\": None,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    # Optional: save a manifest for later reference\n",
    "    manifest_path = os.path.join(out_dir, \"batch_manifest.csv\")\n",
    "    df.to_csv(manifest_path, index=False)\n",
    "    print(f\"\\n‚úî Batch complete. Manifest saved to: {manifest_path}\")\n",
    "    display(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3d6fb-7427-46c7-bb8e-2a0bebf45a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage (edit the path to your folder) ---\n",
    "df_summary = batch_process_videos(\n",
    "     input_dir=\"PATH/TO/VIDEO_FOLDER\",\n",
    "     model_path=MODEL_PATH,    # from model download step\n",
    "     out_dir=out_dir,          # from Section 2.1\n",
    "     recursive=False,          # set True to search subfolders\n",
    " )\n",
    "df_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b2a75b",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Optional: compute simple joint angles\n",
    "\n",
    "Once you have landmarks, you can compute feature engineering targets like **elbow** or **knee angles**. Below is a tiny utility to compute an angle between three named landmarks per frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _angle_between(a, b, c):\n",
    "    # a,b,c are 2D points (x,y) or 3D (x,y,z) ‚Äî here we'll use 2D image coords\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosang = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-9)\n",
    "    cosang = np.clip(cosang, -1.0, 1.0)\n",
    "    return np.degrees(np.arccos(cosang))\n",
    "\n",
    "def compute_joint_angle_csv(csv2d_path: str, joint=(\"left_shoulder\",\"left_elbow\",\"left_wrist\")) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv2d_path)\n",
    "    # wide pivot: columns like x_left_shoulder, y_left_shoulder, etc.\n",
    "    wide = df.pivot_table(index=[\"video\",\"frame\",\"time_ms\"], columns=\"landmark_name\", values=[\"x\",\"y\"])\n",
    "    # helper to get a point\n",
    "    def P(name):\n",
    "        return np.c_[wide[\"x\"][name].values, wide[\"y\"][name].values]\n",
    "    A,B,C = P(joint[0]), P(joint[1]), P(joint[2])\n",
    "    angles = np.array([_angle_between(a,b,c) for a,b,c in zip(A,B,C)])\n",
    "    out = pd.DataFrame({\n",
    "        \"video\": wide.index.get_level_values(\"video\"),\n",
    "        \"frame\": wide.index.get_level_values(\"frame\"),\n",
    "        \"time_ms\": wide.index.get_level_values(\"time_ms\"),\n",
    "        f\"angle_{'_'.join(joint)}\": angles\n",
    "    })\n",
    "    return out\n",
    "\n",
    "# Example (after extraction):\n",
    "# angle_df = compute_joint_angle_csv(\"outputs/yourvideo_pose2d.csv\", (\"left_shoulder\",\"left_elbow\",\"left_wrist\"))\n",
    "# angle_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b8f43",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Notes & best practices\n",
    "\n",
    "- **Timestamps matter:** in `VIDEO` mode you *must* pass `timestamp_ms` that increases with frames; we compute it from frame index and FPS.  \n",
    "- **Tracking saves compute:** in `VIDEO`/`LIVE_STREAM` the task performs pose tracking so the full model isn‚Äôt re-run every frame (helps latency).  \n",
    "- **Out-of-frame landmarks:** 2D normalized `x,y` can be outside `[0,1]` if a joint is off‚Äëscreen; use `visibility` to filter.  \n",
    "- **Model choice:** start with **full**, switch to **lite** for underpowered laptops or large batches, use **heavy** when you need the highest accuracy and can afford the speed.  \n",
    "- **Stride:** a cheap speedup is `frame_stride=2` (¬Ω the frames) or higher.  \n",
    "- **Ethics & consent:** if students process videos of people, teach consent, privacy, and secure storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d86a5ce",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Quick start (edit the path and run)\n",
    "\n",
    "Uncomment one of the calls below and set your paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6746b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- SINGLE VIDEO ---\n",
    "# outputs = extract_pose_from_video(\n",
    "#     video_path=\"PATH/TO/YOUR/VIDEO.mp4\",\n",
    "#     model_path=MODEL_PATH,\n",
    "#     make_annotated_video=True,\n",
    "#     frame_stride=1,   # increase to 2/3 for faster processing\n",
    "# )\n",
    "# print(outputs)\n",
    "\n",
    "# --- DIRECTORY ---\n",
    "# df = batch_process_videos(\n",
    "#     input_dir=\"PATH/TO/FOLDER\",\n",
    "#     model_path=MODEL_PATH,\n",
    "#     pattern=\"*.mp4\",\n",
    "#     out_dir=\"outputs\",\n",
    "#     make_annotated_video=False,\n",
    "#     frame_stride=2,\n",
    "# )\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab198ad",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Troubleshooting\n",
    "- If you see `NoneType` for results, ensure the **model path exists** and your video actually contains a person.  \n",
    "- If you get slowdowns or memory pressure, try `frame_stride=2` or the `\"lite\"` model.  \n",
    "- On some platforms OpenCV MP4 writing may need codecs; if a saved video is empty, try a different `fourcc` (e.g., `cv2.VideoWriter_fourcc(*\"avc1\")`) or install `opencv-python-headless` alternatives.\n",
    "\n",
    "Happy exploring!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LS100_PoseEstimation_MP",
   "language": "python",
   "name": "mediapipeenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
